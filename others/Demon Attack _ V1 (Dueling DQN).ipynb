{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc032920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        if not isinstance(data, tuple) or len(data) != 5:\n",
    "            raise ValueError(f\"Invalid data inserted into SumTree: {data}\")\n",
    "        \n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.n_entries = min(self.n_entries + 1, self.capacity)\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0\n",
    "\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "\n",
    "        # Propagate the change through tree\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "\n",
    "        while True:\n",
    "            left = 2 * parent_idx + 1\n",
    "            right = left + 1\n",
    "\n",
    "            if left >= len(self.tree):  # leaf node\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left]:\n",
    "                    parent_idx = left\n",
    "                else:\n",
    "                    value -= self.tree[left]\n",
    "                    parent_idx = right\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9e27b",
   "metadata": {},
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the buffer.\"\"\"\n",
    "        # Convert states to float32 for consistent dtype\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        next_state = np.array(next_state, dtype=np.float32)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.buffer, min(self.batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to NumPy arrays with correct shapes\n",
    "        states = np.stack(states)                           # Shape: (batch_size, *state_shape)\n",
    "        next_states = np.stack(next_states)\n",
    "        actions = np.array(actions, dtype=np.int32)         # Shape: (batch_size,)\n",
    "        rewards = np.array(rewards, dtype=np.float32)       # Shape: (batch_size,)\n",
    "        dones = np.array(dones, dtype=np.uint8)             # Shape: (batch_size,)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcb700",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cloudpickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_buffers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_prioritized_replay_buffer\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrajectories\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trajectory\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_spec\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/__init__.py:74\u001b[0m\n\u001b[1;32m     70\u001b[0m _ensure_tf_install()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m agents\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bandits\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TF-Agents Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Module importing all agents.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m behavioral_cloning\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m categorical_dqn\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cql\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/behavioral_cloning/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TF-Agents Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"A Behavioral Cloning agent.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbehavioral_cloning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m behavioral_cloning_agent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/behavioral_cloning/behavioral_cloning_agent.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgin\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_converter\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_agent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/__init__.py:20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m substrates\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/substrates/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_loader  \u001b[38;5;66;03m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n\u001b[1;32m     21\u001b[0m jax \u001b[38;5;241m=\u001b[39m lazy_loader\u001b[38;5;241m.\u001b[39mLazyLoader(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m(),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_probability.substrates.jax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:138\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[1;32m    136\u001b[0m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m all_util\u001b[38;5;241m.\u001b[39mremove_undocumented(\u001b[38;5;18m__name__\u001b[39m, _lazy_load \u001b[38;5;241m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:57\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:40\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/__init__.py:31\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"TensorFlow Probability API-unstable package.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mThis package contains potentially useful code which is under active development\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mYou are welcome to try any of this out (and tell us how well it works for you!).\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_batching\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bayesopt\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bijectors\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"TensorFlow Probability experimental Bayesopt package.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acquisition\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[1;32m     20\u001b[0m _allowed_symbols \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macquisition\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/__init__.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcquisitionFunction\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCMCReducer\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianProcessExpectedImprovement\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelExpectedImprovement\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StudentTProcessExpectedImprovement\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/expected_improvement.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Expected Improvement.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normal\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m student_t\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbayesopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acquisition_function\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/distributions/__init__.py:110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpareto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pareto\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PERT\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_cnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PixelCNN\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplackett_luce\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PlackettLuce\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoisson\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Poisson\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/distributions/pixel_cnn.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reparameterization\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorshape_util\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m weight_norm\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPixelCNN\u001b[39;00m(distribution\u001b[38;5;241m.\u001b[39mDistribution):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"The Pixel CNN++ distribution.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m  Pixel CNN++ [(Salimans et al., 2017)][1] models a distribution over image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m       Learning_, 2016. https://arxiv.org/pdf/1601.06759.pdf\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/layers/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense_variational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseReparameterization\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense_variational_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseVariational\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CategoricalMixtureOfOneHotCategorical\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributionLambda\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndependentBernoulli\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/layers/distribution_layer.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Dependency imports\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcloudpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CloudPickler\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cloudpickle'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models,regularizers\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Lambda, Input,Add, Reshape, Activation, Softmax,Multiply, BatchNormalization, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.activations import gelu\n",
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d1d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329c59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemonAttackEnviroment:\n",
    "    def __init__(self, render_mode):\n",
    "        self.render_mode = render_mode if render_mode else None\n",
    "        self.frame_stack = deque(maxlen=4)\n",
    "        self.env = gym.make(\"DemonAttackDeterministic-v4\",render_mode=render_mode,full_action_space=True,frameskip=2)\n",
    "        self.state_dim = (4, 84, 84)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.state = self.reset()\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        if isinstance(state, np.ndarray) and state.ndim == 3 and state.shape[2] == 3:\n",
    "            gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)                                  #convert to grayscale\n",
    "            resized = cv2.resize(gray, (84, 84)) / 255.0                                    #resize and normalise\n",
    "            resized = np.float32(resized)                                                   #convert to float32\n",
    "            return resized                                                                  \n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported state type: {type(state)} with shape {getattr(state, 'shape', None)}\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.frame_stack.clear()\n",
    "        preprocessed = self.preprocess_state(self.state)\n",
    "        for _ in range(4):\n",
    "            self.frame_stack.append(preprocessed)\n",
    "        return np.stack(self.frame_stack, axis=2)\n",
    "\n",
    "    def step(self,action):\n",
    "        self.state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.state = self.preprocess_state(self.state)\n",
    "        self.frame_stack.append(self.state)\n",
    "        return np.stack(self.frame_stack, axis=2), reward, done, info['lives']\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def env_id(self):\n",
    "        return self.env.spec.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71256888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network:\n",
    "    def __init__(self, input_shape, action_size, optimizer='adam'):\n",
    "        tf.random.set_seed(42)  # For reproducibility\n",
    "        self.input_shape = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.optimizer = optimizer\n",
    "        self.model = self.build_network()\n",
    "\n",
    "    def build_network(self):\n",
    "        def dueling_q_values(inputs):\n",
    "            v, a = inputs\n",
    "            a_mean = tf.reduce_mean(a, axis=1, keepdims=True)\n",
    "            return v + (a - a_mean)\n",
    "\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = Conv2D(16, (8, 8),\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(gelu)(x)\n",
    "        x = MaxPooling2D((2,2),strides=2)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        x = Conv2D(32, (4, 4),\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(gelu)(x)\n",
    "        x = MaxPooling2D((2,2))(x)\n",
    "        '''\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(64, (7, 7),\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2,2))(x)\n",
    "        x = Activation(gelu)(x)\n",
    "        # Compute attention scores\n",
    "        attention = Conv2D(1, (1, 1), activation='linear')(x)  # shape: (batch, h, w, 1)\n",
    "        attention = Flatten()(attention)                      # shape: (batch, h * w)\n",
    "        attention = Activation('softmax')(attention)          # soft attention across all locations\n",
    "        attention = Reshape(x.shape[1:3] + (1,))(attention)    # reshape back to (batch, h, w, 1)\n",
    "\n",
    "        # Apply attention\n",
    "        x = Multiply()([x, attention])  # broadcast attention across channels\n",
    "\n",
    "\n",
    "        # Value stream\n",
    "        v = Dense(8,\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        v = Activation(gelu)(v)\n",
    "\n",
    "        v = Dense(1, activation='linear')(v)\n",
    "        '''\n",
    "        # Advantage stream\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(256,\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = Activation(gelu)(x)\n",
    "\n",
    "        x = Dense(self.action_size, activation='linear')(x)\n",
    "\n",
    "        # Combine value and advantage into Q-values\n",
    "        #q_values = Lambda(dueling_q_values)([v, a])\n",
    "\n",
    "        model = models.Model(inputs=inputs, outputs=x)\n",
    "        model.compile(optimizer=self.optimizer, loss='mae')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, state_batch):\n",
    "        return self.model(state_batch, training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e577ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras' has no attribute '__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreplay_buffers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_prioritized_replay_buffer\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrajectories\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trajectory\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/__init__.py:74\u001b[39m\n\u001b[32m     70\u001b[39m _ensure_tf_install()\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m agents\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bandits\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TF-Agents Authors.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"Module importing all agents.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m behavioral_cloning\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m categorical_dqn\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cql\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/behavioral_cloning/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TF-Agents Authors.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"A Behavioral Cloning agent.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbehavioral_cloning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m behavioral_cloning_agent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tf_agents/agents/behavioral_cloning/behavioral_cloning_agent.py:37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgin\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfp\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_converter\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_agents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_agent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/__init__.py:20\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m substrates\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/substrates/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_loader  \u001b[38;5;66;03m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n\u001b[32m     21\u001b[39m jax = lazy_loader.LazyLoader(\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m(),\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtensorflow_probability.substrates.jax\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:138\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[32m    136\u001b[39m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[32m    137\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[32m    141\u001b[39m all_util.remove_undocumented(\u001b[34m__name__\u001b[39m, _lazy_load + _maybe_nonlazy_load)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:57\u001b[39m, in \u001b[36mLazyLoader.__dir__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m   module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:40\u001b[39m, in \u001b[36mLazyLoader._load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m   \u001b[38;5;28mself\u001b[39m._on_first_access = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_module_globals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     42\u001b[39m   \u001b[38;5;28mself\u001b[39m._parent_module_globals[\u001b[38;5;28mself\u001b[39m._local_name] = module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/__init__.py:31\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"TensorFlow Probability API-unstable package.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mThis package contains potentially useful code which is under active development\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03mYou are welcome to try any of this out (and tell us how well it works for you!).\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_batching\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bayesopt\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bijectors\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"TensorFlow Probability experimental Bayesopt package.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acquisition\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[32m     20\u001b[39m _allowed_symbols = [\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33macquisition\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/__init__.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcquisitionFunction\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCMCReducer\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianProcessExpectedImprovement\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelExpectedImprovement\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpected_improvement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StudentTProcessExpectedImprovement\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/expected_improvement.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Expected Improvement.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normal\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m student_t\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesopt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01macquisition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acquisition_function\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/distributions/__init__.py:110\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpareto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pareto\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PERT\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpixel_cnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PixelCNN\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplackett_luce\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PlackettLuce\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpoisson\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Poisson\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/distributions/pixel_cnn.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reparameterization\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorshape_util\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m weight_norm\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPixelCNN\u001b[39;00m(distribution.Distribution):\n\u001b[32m     37\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"The Pixel CNN++ distribution.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m  Pixel CNN++ [(Salimans et al., 2017)][1] models a distribution over image\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m       Learning_, 2016. https://arxiv.org/pdf/1601.06759.pdf\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/layers/__init__.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense_variational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseReparameterization\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense_variational_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseVariational\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CategoricalMixtureOfOneHotCategorical\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributionLambda\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribution_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndependentBernoulli\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow_probability/python/layers/distribution_layer.py:68\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_tuple\n\u001b[32m     50\u001b[39m __all__ = [\n\u001b[32m     51\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCategoricalMixtureOfOneHotCategorical\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     52\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mDistributionLambda\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mVariationalGaussianProcess\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     65\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__internal__\u001b[49m.utils.register_symbolic_tensor_type(dtc._TensorCoercible)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_event_size\u001b[39m(event_shape, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     72\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Computes the number of elements in a tensor with shape `event_shape`.\u001b[39;00m\n\u001b[32m     73\u001b[39m \n\u001b[32m     74\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m    a scalar tensor.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras._tf_keras.keras' has no attribute '__internal__'"
     ]
    }
   ],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, alpha=0.6, beta=0.4, beta_increment=1e-4, epsilon=1e-6):\n",
    "        self.tree = SumTree(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha  # how much prioritization to use (0 = none, 1 = full)\n",
    "        self.beta = beta    # importance sampling weight correction\n",
    "        self.beta_increment = beta_increment\n",
    "        self.epsilon = epsilon  # small value to avoid zero priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree.data)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Create experience tuple\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        # Optional: sanity check on experience shape\n",
    "        if not isinstance(data, tuple) or len(data) != 5:\n",
    "            raise ValueError(f\"Invalid data inserted into SumTree: {data}\")\n",
    "\n",
    "        # Set initial priority (will be updated later during learning)\n",
    "        priority = 0.6\n",
    "\n",
    "        # Add to the sum tree with priority^alpha\n",
    "        self.tree.add(priority ** self.alpha, data)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        priorities = []\n",
    "\n",
    "        total_priority = self.tree.total_priority()\n",
    "        if total_priority == 0:\n",
    "            raise ValueError(\"Total priority is zero. SumTree might be empty or improperly updated.\")\n",
    "\n",
    "        segment = total_priority / self.batch_size\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = self.batch_size * 5  # avoid infinite loops\n",
    "        while len(batch) < self.batch_size and attempts < max_attempts:\n",
    "            i = len(batch)\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = np.random.uniform(a, b)\n",
    "            idx, priority, data = self.tree.get_leaf(s)\n",
    "\n",
    "            # Skip if data is not valid or priority is zero (unfilled leaf)\n",
    "            if priority == 0 or not isinstance(data, tuple) or len(data) != 5:\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(priority)\n",
    "            attempts += 1\n",
    "\n",
    "        if len(batch) < self.batch_size:\n",
    "            raise RuntimeError(f\"Failed to sample a full batch. Collected {len(batch)} out of {self.batch_size}.\")\n",
    "\n",
    "        # Importance sampling weights\n",
    "        priorities = np.array(priorities, dtype=np.float32)\n",
    "        sampling_probs = priorities / (total_priority + 1e-8)\n",
    "        sampling_probs = np.maximum(sampling_probs, 1e-6)\n",
    "        #print(\"Sampling props normalised and clipped\",sampling_probs[0:5])\n",
    "\n",
    "        weights = (self.tree.n_entries * sampling_probs) ** (-self.beta)\n",
    "        #print('weights after multiplying by sampling probs:', weights[0:5] )\n",
    "        weights = np.clip(weights, a_min=1e-3, a_max=10.0)  \n",
    "\n",
    "        weights = weights /  max(weights)\n",
    "        #print('weights after dividing by max:', weights[0:5] )\n",
    "\n",
    "        # Unpack batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.stack(states), np.array(actions), np.array(rewards), \n",
    "                np.stack(next_states), np.array(dones), np.array(idxs), np.array(weights, dtype=np.float32))\n",
    "\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            min_priority = 1e-6\n",
    "            priority = max(priority, min_priority)\n",
    "            self.tree.update(idx, priority)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35046a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, buffer_size, batch_size, episodes, input_shape = (84,84,4), action_size=2,\n",
    "                 gamma = 0.9, epsilon = 0.9, epsilon_min = 0.1,epsilon_decay = 0.999, learning_rate = 0.0001, \n",
    "                 tau = 0.001, optimiser = 'Adam', environment = None, update_frequency=100, mode = 'Train'):\n",
    "        \n",
    "        set_seed(42) # Seeding for result reproducibility\n",
    "        self.fixed_states = []\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.step = 0\n",
    "\n",
    "        self.lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=self.learning_rate,\n",
    "        decay_steps=self.episodes,\n",
    "        decay_rate=0.999,\n",
    "        staircase=True\n",
    "        )\n",
    "        \n",
    "        # Optimiser value handling\n",
    "        if isinstance(optimiser, str):\n",
    "            if optimiser == 'Adam':\n",
    "                self.optimizer = Adam(learning_rate=self.lr_schedule, clipnorm=1.0)\n",
    "            elif optimiser == 'SGD':\n",
    "                self.optimizer = SGD(learning_rate=self.lr_schedule)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported optimizer string.\")\n",
    "        else:\n",
    "            self.optimizer = optimiser  # Assume it's already a compiled tf optimizer object\n",
    "\n",
    "        self.qnet = Q_Network(self.input_shape, self.action_size, self.optimizer)\n",
    "        self.target_net = Q_Network(self.input_shape, self.action_size, self.optimizer)\n",
    "        self.target_net.model.set_weights(self.qnet.model.get_weights())\n",
    "\n",
    "        \n",
    "        q_weights = self.qnet.model.get_weights()\n",
    "        target_weights = self.target_net.model.get_weights()\n",
    "\n",
    "        for qw, tw in zip(q_weights, target_weights):\n",
    "            assert np.array_equal(qw, tw), \"Weights differ!\"\n",
    "\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(buffer_size=buffer_size, batch_size=self.batch_size)\n",
    "\n",
    "        #Environment Parameter Handling\n",
    "        self.env = environment if environment is not None else gym.make(\"Blackjack-v1\", sab=True)\n",
    "        self.env_name = self.env.env_id() # Used to determine env type\n",
    "        self.collect_fixed_states()\n",
    "\n",
    "\n",
    "\n",
    "    ########################################################################################################################\n",
    "    # Greedy epsilon function for action selection (Optimal vs Random Choice) ##############################################\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = np.expand_dims(np.array(state, dtype=np.float32), axis=0)\n",
    "            return int(np.argmax(self.qnet.predict(state)))\n",
    "\n",
    "    ########################################################################################################################\n",
    "    # Updates Target Network from the Q_Network weights at a rate of Tau ###################################################\n",
    "    def soft_update_target_network(self):\n",
    "        qnet_weights = self.qnet.model.get_weights()\n",
    "        target_net_weights = self.target_net.model.get_weights()\n",
    "\n",
    "        updated_weights = [\n",
    "            self.tau * q_w + (1 - self.tau) * t_w\n",
    "            for q_w, t_w in zip(qnet_weights, target_net_weights)\n",
    "        ]\n",
    "\n",
    "        self.target_net.model.set_weights(updated_weights)\n",
    "    \n",
    "    def collect_fixed_states(self, num_states=1000):\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(num_states):\n",
    "            action = self.env.env.action_space.sample()\n",
    "            next_obs, _, done, _ = self.env.step(action)\n",
    "            self.fixed_states.append(obs)\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "\n",
    "    def evaluate_q_on_fixed_states(self):\n",
    "        q_values = []\n",
    "        for state in self.fixed_states:\n",
    "            state_array = np.expand_dims(np.array(state, dtype=np.float32), axis=0)\n",
    "            q = self.qnet.predict(state_array)  # shape: (1, action_size)\n",
    "            max_q = np.max(q)\n",
    "            q_values.append(max_q)\n",
    "        return np.mean(q_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self):\n",
    "    # Sample a batch of experiences from the prioritized replay buffer\n",
    "        states, actions, rewards, next_states, dones, weights, indices = self.replay_buffer.sample()\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_net.model(next_states)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "        targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "        # Prepare indices to gather Q-values of chosen actions\n",
    "        action_indices = tf.stack([tf.range(self.batch_size), actions], axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass through the main Q-network\n",
    "            q_values = self.qnet.model(states)\n",
    "            chosen_q_values = tf.gather_nd(q_values, action_indices)\n",
    "\n",
    "            # Compute TD errors\n",
    "            td_errors = targets - chosen_q_values\n",
    "\n",
    "            # Weighted MSE loss using importance-sampling weights\n",
    "            clipped_td_errors = tf.clip_by_value(td_errors, -1.0, 1.0)\n",
    "\n",
    "            loss = tf.reduce_mean(tf.square(clipped_td_errors) * weights)\n",
    "            #print('weights: ',np.mean(weights))\n",
    "            #print('loss:',loss)\n",
    "\n",
    "        # Apply gradients\n",
    "        grads = tape.gradient(loss, self.qnet.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.qnet.model.trainable_variables))\n",
    "\n",
    "        # Update priorities in the replay buffer\n",
    "        indices = np.array(indices, dtype=np.int32)\n",
    "        new_priorities = np.array(np.abs(td_errors.numpy()), dtype=np.float32)\n",
    "        clipped_priorities = np.clip(new_priorities, a_min=1e-6, a_max=10.0)\n",
    "\n",
    "        self.replay_buffer.update_priorities(indices, clipped_priorities)\n",
    "\n",
    "\n",
    "        # Return loss and mean Q-value for logging\n",
    "        mean_q_value = tf.reduce_mean(chosen_q_values).numpy()\n",
    "        return loss.numpy(), mean_q_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.episode_rewards = []\n",
    "        self.mean_losses = []\n",
    "        self.mean_exp_return = []\n",
    "        self.q_eval_track = []\n",
    "        print(\"Using device:\", tf.test.gpu_device_name())\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            old_lives = 4 # or set to 5 initially if known\n",
    "\n",
    "            i = 0\n",
    "            episode_losses = []\n",
    "            episode_q_values = []\n",
    "            total_raw_reward = 0\n",
    "            total_scaled_reward = 0\n",
    "            lives_lost = 0\n",
    "            while not done:\n",
    "                action = self.select_action(state, self.epsilon)\n",
    "                next_state, raw_reward, done, new_lives = self.env.step(action)\n",
    "                \n",
    "                # Only compare lives after first step\n",
    "                #  Custom reward shaping logic\n",
    "                reward_scale = 100.0\n",
    "                death_penalty = -0.5\n",
    "                survival_bonus = 0.005\n",
    "\n",
    "\n",
    "                if raw_reward > 0:\n",
    "                    reward = raw_reward / reward_scale\n",
    "                elif old_lives > new_lives:\n",
    "                    reward = death_penalty\n",
    "                else:\n",
    "                    reward = survival_bonus\n",
    "                    \n",
    "                total_raw_reward += raw_reward\n",
    "                total_scaled_reward += reward\n",
    "                lives_lost += int(old_lives > new_lives)\n",
    "                old_lives = new_lives\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                i+=1\n",
    "                # Ensure learning only starts after the buffer is at least full once\n",
    "                if (\n",
    "                    i % self.update_frequency == 0\n",
    "                    and self.replay_buffer.tree.n_entries == self.replay_buffer.tree.capacity  # or .n_entries if you use it\n",
    "                    and len(self.replay_buffer) >= self.batch_size\n",
    "                ):\n",
    "                    loss, mean_q = self.training_step()\n",
    "                    episode_losses.append(loss)\n",
    "                    episode_q_values.append(mean_q)\n",
    "\n",
    "\n",
    "                        # Update priorities in the PER buffer\n",
    "\n",
    "\n",
    "            avg_q_eval = self.evaluate_q_on_fixed_states()\n",
    "            self.q_eval_track.append(avg_q_eval)\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.mean_losses.append(np.mean(episode_losses) if episode_losses else 0)\n",
    "            self.mean_exp_return.append(np.mean(episode_q_values) if episode_q_values else 0)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(24, 4))\n",
    "            axs[0].plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "            axs[0].set_title(\"Episode Reward\")\n",
    "            axs[1].plot(self.mean_losses, label=\"Mean Loss\", color='orange')\n",
    "            axs[1].set_title(\"Mean Loss\")\n",
    "            axs[2].plot(self.mean_exp_return, label=\"Mean Q-Value\", color='green')\n",
    "            axs[2].set_title(\"Mean Exp return\")\n",
    "            axs[3].plot(self.q_eval_track, label=\"Avg Max Q (Fixed States)\", color='purple')\n",
    "            axs[3].set_title(\"Diagnostic Q-Metric\")\n",
    "            for ax in axs:\n",
    "                ax.set_xlabel(\"Episode\")\n",
    "                ax.grid(True)\n",
    "                ax.legend()\n",
    "            plt.suptitle(\"Training Metrics\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Episode: {episode}/{self.episodes}, Steps: {i}, \"\n",
    "                f\"Total Reward: {total_reward:.2f}, \"\n",
    "                f\"Mean Loss: {np.mean(episode_losses):.4f}, \"\n",
    "                f\"Mean Exp-Q: {np.mean(episode_q_values):.4f}, \"\n",
    "                f\"Average State (Q): {avg_q_eval:.4f}, \"\n",
    "                f\"Epsilon: {self.epsilon:.2f}\")\n",
    "            print(f\"Raw: {total_raw_reward}, Scaled: {total_scaled_reward}, Lives Lost: {lives_lost}\")\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                self.qnet.model.save_weights('./saved_model.weights.h5')\n",
    "\n",
    "    def test(self, weights_path = ''):\n",
    "        self.episode_rewards = []\n",
    "        self.qnet.model.load_weights(weights_path)\n",
    "        self.q_eval_track = []\n",
    "        print(\"Using device:\", tf.test.gpu_device_name())\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            old_lives = 4 # or set to 5 initially if known\n",
    "\n",
    "            i = 0\n",
    "            total_raw_reward = 0\n",
    "            total_scaled_reward = 0\n",
    "            lives_lost = 0\n",
    "            while not done:\n",
    "                action = self.select_action(state, 0)\n",
    "                next_state, raw_reward, done, new_lives = self.env.step(action)\n",
    "                \n",
    "                # Only compare lives after first step\n",
    "                #  Custom reward shaping logic\n",
    "                reward_scale = 100.0\n",
    "                death_penalty = -0.5\n",
    "                survival_bonus = 0.005\n",
    "\n",
    "\n",
    "                if raw_reward > 0:\n",
    "                    reward = raw_reward / reward_scale\n",
    "                elif old_lives > new_lives:\n",
    "                    reward = death_penalty\n",
    "                else:\n",
    "                    reward = survival_bonus\n",
    "                    \n",
    "                total_raw_reward += raw_reward\n",
    "                total_scaled_reward += reward\n",
    "                lives_lost += int(old_lives > new_lives)\n",
    "                old_lives = new_lives\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                i+=1\n",
    "\n",
    "\n",
    "\n",
    "            avg_q_eval = self.evaluate_q_on_fixed_states()\n",
    "            self.q_eval_track.append(avg_q_eval)\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            clear_output(wait=True)\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 2))\n",
    "            axs[0].plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "            axs[0].set_title(\"Episode Reward\")\n",
    "            axs[1].plot(self.q_eval_track, label=\"Avg Max Q (Fixed States)\", color='purple')\n",
    "            axs[1].set_title(\"Diagnostic Q-Metric\")\n",
    "            for ax in axs:\n",
    "                ax.set_xlabel(\"Episode\")\n",
    "                ax.grid(True)\n",
    "                ax.legend()\n",
    "            plt.suptitle(\"Training Metrics\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Episode: {episode}/{self.episodes}, Steps: {i}, \"\n",
    "                f\"Total Reward: {total_reward:.2f}, \"\n",
    "                f\"Average State (Q): {avg_q_eval:.4f}, \")\n",
    "            print(f\"Raw: {total_raw_reward}, Scaled: {total_scaled_reward}, Lives Lost: {lives_lost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b344852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 20:50:12.948286: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-24 20:50:12.948303: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/lj/fj9nh1zs15sgfrzmzkrby7cw0000gn/T/ipykernel_20186/756785953.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m                   \u001b[32m0.0005\u001b[39m,                                                 \u001b[38;5;66;03m#Tau\u001b[39;00m\n\u001b[32m     13\u001b[39m                   \u001b[33m'Adam'\u001b[39m,                                               \u001b[38;5;66;03m#Optimiser\u001b[39;00m\n\u001b[32m     14\u001b[39m                   env,                                                  \u001b[38;5;66;03m#Environment\u001b[39;00m\n\u001b[32m     15\u001b[39m                   1)                                                    #Train Frquency \n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m Agent1.train()\n",
      "\u001b[32m/var/folders/lj/fj9nh1zs15sgfrzmzkrby7cw0000gn/T/ipykernel_20186/2679606915.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m                     i % self.update_frequency == \u001b[32m0\u001b[39m\n\u001b[32m    211\u001b[39m                     \u001b[38;5;28;01mand\u001b[39;00m self.replay_buffer.tree.n_entries == self.replay_buffer.tree.capacity  \u001b[38;5;66;03m# or .n_entries if you use it\u001b[39;00m\n\u001b[32m    212\u001b[39m                     \u001b[38;5;28;01mand\u001b[39;00m len(self.replay_buffer) >= self.batch_size\n\u001b[32m    213\u001b[39m                 ):\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m                     loss, mean_q = self.training_step()\n\u001b[32m    215\u001b[39m                     episode_losses.append(loss)\n\u001b[32m    216\u001b[39m                     episode_q_values.append(mean_q)\n\u001b[32m    217\u001b[39m \n",
      "\u001b[32m/var/folders/lj/fj9nh1zs15sgfrzmzkrby7cw0000gn/T/ipykernel_20186/2679606915.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[38;5;66;03m#print('loss:',loss)\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m         \u001b[38;5;66;03m# Apply gradients\u001b[39;00m\n\u001b[32m    136\u001b[39m         grads = tape.gradient(loss, self.qnet.model.trainable_variables)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         self.optimizer.apply_gradients(zip(grads, self.qnet.model.trainable_variables))\n\u001b[32m    138\u001b[39m \n\u001b[32m    139\u001b[39m         \u001b[38;5;66;03m# Update priorities in the replay buffer\u001b[39;00m\n\u001b[32m    140\u001b[39m         indices = np.array(indices, dtype=np.int32)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    382\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    384\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    444\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m                 grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m             \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m             self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    449\u001b[39m             \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    450\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    451\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    507\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    508\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    509\u001b[39m \n\u001b[32m    510\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m             self._backend_update_step(\n\u001b[32m    512\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    513\u001b[39m             )\n\u001b[32m    514\u001b[39m \n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/optimizers/adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    135\u001b[39m         self.assign_add(\n\u001b[32m    136\u001b[39m             v,\n\u001b[32m    137\u001b[39m             ops.multiply(\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m                 ops.subtract(ops.square(gradient), v), \u001b[32m1\u001b[39m - self.beta_2\n\u001b[32m    139\u001b[39m             ),\n\u001b[32m    140\u001b[39m         )\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.amsgrad:\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/ops/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   6299\u001b[39m         Output tensor, the square of `x`.\n\u001b[32m   6300\u001b[39m     \"\"\"\n\u001b[32m   6301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m   6302\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Square().symbolic_call(x)\n\u001b[32m-> \u001b[39m\u001b[32m6303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.numpy.square(x)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/backend/tensorflow/sparse.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, *args, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m             return tf.IndexedSlices(\n\u001b[32m    384\u001b[39m                 func(x.values, *args, **kwargs), x.indices, x.dense_shape\n\u001b[32m    385\u001b[39m             )\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(x, *args, **kwargs)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   2590\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m square(x):\n\u001b[32m   2591\u001b[39m     x = convert_to_tensor(x)\n\u001b[32m   2592\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m standardize_dtype(x.dtype) == \u001b[33m\"bool\"\u001b[39m:\n\u001b[32m   2593\u001b[39m         x = tf.cast(x, \u001b[33m\"int32\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.square(x)\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m     89\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m     91\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, name)\u001b[39m\n\u001b[32m  12056\u001b[39m         _ctx, \"Square\", name, x)\n\u001b[32m  12057\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  12058\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  12059\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m12060\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  12061\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12062\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12063\u001b[39m       _result = _dispatcher_for_square(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = DemonAttackEnviroment(None)\n",
    "Agent1 = DQNAgent(100,                                                  #Buffer Size\n",
    "                  64,                                                   #Batch Size\n",
    "                  1000,                                                 #Episodes\n",
    "                  (84,84,4),                                            #Input Shape\n",
    "                  18,                                                   #Action Size\n",
    "                  0.95,                                                 #Gamma\n",
    "                  1,                                                    #Epsilon\n",
    "                  0.1,                                                  #Epslon Minimum\n",
    "                  0.90,                                                 #Epslon Decay\n",
    "                  0.001,                                                 #Learning Rate\n",
    "                  0.0005,                                                 #Tau\n",
    "                  'Adam',                                               #Optimiser  \n",
    "                  env,                                                  #Environment\n",
    "                  1)                                                    #Train Frquency \n",
    "Agent1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7fa21",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m Test_env = DemonAttackEnviroment(\u001b[33m'\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Test_Agent = \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                 \u001b[49m\u001b[38;5;66;43;03m#Buffer Size\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                   \u001b[49m\u001b[38;5;66;43;03m#Batch Size\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m#Episodes\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m                  \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m84\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m84\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m#Input Shape\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                   \u001b[49m\u001b[38;5;66;43;03m#Action Size\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m#Gamma\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                    \u001b[49m\u001b[38;5;66;43;03m#Epsilon\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m#Epslon Minimum\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                 \u001b[49m\u001b[38;5;66;43;03m#Epslon Decay\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                 \u001b[49m\u001b[38;5;66;43;03m#Learning Rate\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m#Tau\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mRMSprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#Optimiser  \u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mTest_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m#Environment\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m                  \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m     16\u001b[39m Test_Agent.test(\u001b[33m'\u001b[39m\u001b[33m./saved_model.weights.h5\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mDQNAgent.__init__\u001b[39m\u001b[34m(self, buffer_size, batch_size, episodes, input_shape, action_size, gamma, epsilon, epsilon_min, epsilon_decay, learning_rate, tau, optimiser, environment, update_frequency, mode)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.env = environment \u001b[38;5;28;01mif\u001b[39;00m environment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gym.make(\u001b[33m\"\u001b[39m\u001b[33mBlackjack-v1\u001b[39m\u001b[33m\"\u001b[39m, sab=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.env_name = \u001b[38;5;28mself\u001b[39m.env.env_id() \u001b[38;5;66;03m# Used to determine env type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_fixed_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mDQNAgent.collect_fixed_states\u001b[39m\u001b[34m(self, num_states)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_states):\n\u001b[32m     89\u001b[39m     action = \u001b[38;5;28mself\u001b[39m.env.env.action_space.sample()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     next_obs, _, done, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mself\u001b[39m.fixed_states.append(obs)\n\u001b[32m     92\u001b[39m     obs = next_obs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mDemonAttackEnviroment.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m,action):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28mself\u001b[39m.state, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28mself\u001b[39m.preprocess_state(\u001b[38;5;28mself\u001b[39m.state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RLgym/lib/python3.11/site-packages/ale_py/env/gym.py:256\u001b[39m, in \u001b[36mAtariEnv.step\u001b[39m\u001b[34m(self, action_ind)\u001b[39m\n\u001b[32m    254\u001b[39m reward = \u001b[32m0.0\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     reward += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43male\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m is_terminal = \u001b[38;5;28mself\u001b[39m.ale.game_over(with_truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    258\u001b[39m is_truncated = \u001b[38;5;28mself\u001b[39m.ale.game_truncated()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Test_env = DemonAttackEnviroment('human')\n",
    "Test_Agent = DQNAgent(1000000,                                                 #Buffer Size\n",
    "                  32,                                                   #Batch Size\n",
    "                  1000,                                                  #Episodes\n",
    "                  (84,84,4),                                            #Input Shape\n",
    "                  18,                                                   #Action Size\n",
    "                  0.9,                                                  #Gamma\n",
    "                  1,                                                    #Epsilon\n",
    "                  0.1,                                                  #Epslon Minimum\n",
    "                  0.99,                                                 #Epslon Decay\n",
    "                  0.01,                                                 #Learning Rate\n",
    "                  0.001,                                                  #Tau\n",
    "                  RMSprop(learning_rate=0.0001),                        #Optimiser  \n",
    "                  Test_env,                                                  #Environment\n",
    "                  1)  \n",
    "Test_Agent.test('./saved_model.weights.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
